---
layout: post
title: Mixture of Expert
---

Mixture of expert is an architecture made famous my mistral models mixture of expert. The main idea behind those is dividing for better classifation.

# MOE

Mixture of experts (MoE) is a machine learning approach that divides an artificial intelligence (AI) model into separate sub-networks (or “experts”), each specializing in a subset of the input data, to jointly perform a task.

<figure>
  <img src="../images/moe.avif" width="200" height="200" class=center/>
  <figcaption class=center>Fig.1 - Mixture of expert.</figcaption>
</figure>

- the concept of MOE is related to the concept of conditional computation. the main objective &rarr; improve model capacity without a proporitanl compute increases.

- another name of this idea is sparse gating.

- what MOE solve: when model size and dataset size increases, the computation cost increases quadratically with these features.
  In order to achieve faster training and faster inference, MOE activates only "some of the weights" called experts. the decision on which expert to
  activate is organized by a gating mechanism or a load balancer. the main problem solved is a **computational one**

- the load balancing part can disributed loads to different experts in a sparse or continious fashion.

- why do we want to increase size of model ? &rarr; gain better predicition power, better expressivity

- so the main problematic is how to scale huge model using conditional computation in order to get faster and stable training.

- trying to achieve this is difficult since GPU computation accross different device exhibit communication slow down.

- in 2017 paper; this concept is used with LSTM architecture for machine translation. at each step in the LSTM forward pass a subsample
  of experts is selected.

- expert tends to specialize based on syntax and semantics

- “The flat MoE layers use k = 4 and the hierarchical MoE models use k = 2 at each level of the gating network. Thus, each input is processed by exactly 4 experts in each MoE layer. Each expert in the MoE layer is a feed forward network with one hidden layer of size 2048 and ReLU activation” ([Shazeer et al., 2017, p. 17](zotero://select/library/items/IFIWI6Y3)) ([pdf](zotero://open-pdf/library/items/BNXKN8WH?page=17&annotation=XKKF4K7B))

- the authors speaks about `convolutional application of the moe allows for different gating decisions at each postition in the text` (need to understand this)

- if a number of expert is very big we can use a hierachical MOE where experts are aranged on 2 levels where at each level a gating mechanism is implemented.

## Gating Mechanism

the gating mechanism is sparse. 2 main forms are implemented:

- $$G(x) = SoftMax(x . W_g)$$
- $$G(x) = SoftMax(KeepTopK(H(x), k))$$
  $$H(x) = (x. W_g)_i + StandardNormal() . Softplus((x . W_{noise})_i)$$
  $$
  KeepTopK(v, k)_i = \begin{cases}
                          v_i \quad \text{if $v_i$ is in the top $k$ elements of $v$} \\
                          - \infty  \quad \text{otherwise} \\
                          \end{cases}
  $$
- one issue though is that the gating needs to be balanced since large weights are always associated to few experts



## Training

model architecture: 
    - each batch &rarr; 300 000 words 
    - 10 epochs (27 000 steps)
    - training time 12 - 16 hours 
    - optimizer adam
    - $w_{importance} = 0.1  
    = $w_{load}  = 0,1