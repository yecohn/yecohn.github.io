---
layout: post
title: Mixture of Expert 
---

Mixture of expert is an architecture made famous my mistral models mixture of expert. The main idea behind those is dividing for better classifation. 




# MOE
Mixture of experts (MoE) is a machine learning approach that divides an artificial intelligence (AI) model into separate sub-networks (or “experts”), each specializing in a subset of the input data, to jointly perform a task.

<figure>
  <img src="../images/moe.avif" width="200" height="200" />
  <figcaption>Fig.1 - Mixture of expert.</figcaption>
</figure>




Mixture of Experts architectures enable large-scale models, even those comprising many billions of parameters, to greatly reduce computation costs during pre-training and achieve faster performance during inference time. Broadly speaking, it achieves this efficiency through selectively activating only the specific experts needed for a given task, rather than activating the entire neural network for every task.

Though much of the modern implementation of mixture of experts setups was developed over (roughly) the past decade, the core premise behind MoE models originates from the 1991 paper “Adaptive Mixture of Local Experts.” The paper proposed training an AI system composed of separate networks that each specialized in a different subset of training cases. This entailed training both the “expert networks” themselves and a gating network that determines which expert should be used for each subtask. When compared to an analogous conventional model, the authors found that their experimental model was significantly faster to train: it reached the target accuracy threshold in half as many training epochs as the conventional model.1

In recent years, as the leading deep learning models used for generative AI have grown increasingly large and computationally demanding, mixture of experts offer a means to address the tradeoff between the greater capacity of larger models and the greater efficiency of smaller models. This has been most notably explored in the field of natural language processing (NLP): some leading large language models (LLMs) like Mistral’s Mixtral 8x7B and (according to some reports) OpenAI’s GPT-4,2 have employed MoE architecture.
